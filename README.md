All the script should be run inside the root directory of this project.
## Data
`data` contains all the generated data formatted as a dictionary where each key is canonical words, corresponding to multiple noisy word forms. 
The datasets are named by `{sentiment}-{noise-type}.json`. For example, `neg-typos.json` contains words with negative sentiment, and its noisy words are generated by typos.
`data_generator` contains the scripts to genrate these noisy data.


## Corruption result
$ python check_result.py --help # see help information for valid argument parameters
We save all the results in the `data` directory. You can check them by the script `check_result.py`.
```
$ python check_result.py --help # see help information for valid argument parameters
$ python check_result.py --dataset_name neg-typos --model_name bert-yelp
```
We also provide Python codes in `check_result.ipynp` to load the dataframe and further analyze the result.

To reproduce our results, you can use the script `word_corruption.py`.
```
$ dataset_name=neg-typos
$ model_name=bert-base-uncased-SST-2
$ python word_corruption.py --dataset_name $dataset_name --model_name $model_name 
```
The script will generate the result as Pandas dataframe.


## (Optional) Evaluating new Models
Either subword tokenizers, PLMs or data used in our experiments can be easily accessible in provided Python module `resource` as a dictionary.

```
import resource
dataset_name = "neg-typos"
plm_name = "bert"
tokenizer = resource.hf_tokenizers[plm_name]
plm = resource.hf_models["-".join(plm_name, dataset_name)]
data = resource.datasets[dataset_name]
```

To add new huggingface models, you only need to add a key-value pair in the `hf_model_names` and then all the experiment can be regenerated on this new models.